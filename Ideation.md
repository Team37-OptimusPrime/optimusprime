# Project Ideation (Version 0.1)

## 1. Background & Motivation
- 최근 인공지능(AI) 워크로드가 늘어나면서 클라우드/엣지/온디바이스 환경에서의 자원 관리와 효율성 확보 필요성이 커지고 있다.
- 우리 팀은 [AI/운영체제/자원관리/컴퓨터 아키텍처] 분야를 중심으로 연구 주제를 탐색 중이다.
---

## 2. Brainstormed Ideas
1. **RISC-V 기반 온디바이스 AI 경량화 시스템 (메인 후보)**
  - 오픈소스 ISA(RISC-V)를 활용한 맞춤형 AI 친화적 아키텍처 설계
  - 온디바이스 환경에서 성능–전력 균형 확보
  - 경량화된 AI 모델 효율적 실행을 위한 저수준 최적화 연구
  - 
2. **AI 워크로드를 위한 이기종 자원 운영체제 설계 및 오케스트레이션 기법 연구 (메인 후보)**
  - GPU/NPU/CPU/FPGA 등 이기종 자원 관리
  - 커널 레벨에서 스케줄링·메모리 관리 최적화
  - 성능–전력 효율화 → AI 친화적 운영체제 방향

3. **자원 분리형 데이터센터 OS 설계 및 동적 자원 관리 기법 연구 (메인 후보)**
  - CPU/메모리/GPU/스토리지를 분리(Disaggregated Datacenter, DDC)
  - 필요에 따라 자원을 동적으로 할당·회수
  - 네트워크 지연 문제와 자원 스케줄링 정책 연구

4. **GPU 워크로드 기반 자원 관리 및 스케줄링 최적화 (예비 후보)**
  - GPU 중심의 AI 워크로드 특성을 반영한 운영체제 레벨 자원 관리 연구
  - GPU 활용률을 극대화하기 위한 동적 스케줄링 정책 설계
  - 성능–전력 효율을 동시에 고려한 최적화 기법 개발
  - 대규모 AI 모델 실행 시 GPU 자원 병목 현상 완화 목표

5. **GPU 메모리 상태 기반 실시간 LLM 레이어 재분할 적응형 시스템 (예비 후보)**
  - GPU 메모리 점유율에 따라 LLM(Large Language Model)의 레이어를 실시간으로 재배치/재분할
  - 메모리 부족 상황에서도 안정적으로 대규모 모델 추론 가능
  - 적응형(adaptive) 자원 관리 기법으로 성능–자원 균형 유지
  - GPU 메모리 활용 극대화 및 LLM 추론 효율 향상

6. **[EdgeAI] Neural Network Partitioning for Multi-Access Edge Computing (네트워크 지연과 디바이스 성능에 따라 신경망을 동적으로 분할해서 클라우드-엣지 간 최적 배치)**

7. **[최적화] 여러 추론 작업이 GPU를 공유할 때 실시간 부하에 따라 리소스를 동적 재할당하는 시스템**

8. **[AI(모델학습),시스템 최적화]자체 학습 데이터 생성 및 경량화된 AI 모델을 활용한 '이상 감지 시스템' 개발**

9. **[AI,OS] AI 기반 운영체제 이상 탐지 및 자율 튜닝**

10. **[AI/최적화/분산 스케줄링] AI inference workload에 특화된 serverless 스케줄링 기법 연구(or severless computing 최적화 연구)**
---

## 3. Next Steps
- 팀 내 투표/논의 및 교수님 조언을 통해 메인 주제 확정
- 각 아이디어별 장단점 및 필요 리소스 분석 예정
