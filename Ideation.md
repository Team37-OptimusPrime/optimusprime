# Project Ideation (Version 0.9)

## 1. Background & Motivation
- 최근 인공지능(AI) 워크로드가 늘어나면서 클라우드/엣지/온디바이스 환경에서의 자원 관리와 효율성 확보 필요성이 커지고 있다.
- 우리 팀은 [AI/운영체제/자원관리/컴퓨터 아키텍처] 분야를 중심으로 연구 주제를 탐색 중이다.
---

## 2. Brainstormed Ideas
1. **RISC-V 기반 온디바이스 AI 경량화 시스템 (메인 후보)**
  - 오픈소스 ISA(RISC-V)를 활용한 맞춤형 AI 친화적 아키텍처 설계
  - 온디바이스 환경에서 성능–전력 균형 확보
  - 경량화된 AI 모델 효율적 실행을 위한 저수준 최적화 연구

    
2. **AI 워크로드를 위한 이기종 자원 운영체제 설계 및 오케스트레이션 기법 연구 (메인 후보)**
  - GPU/NPU/CPU/FPGA 등 이기종 자원 관리
  - 커널 레벨에서 스케줄링·메모리 관리 최적화
  - 성능–전력 효율화 → AI 친화적 운영체제 방향


3. **자원 분리형 데이터센터 OS 설계 및 동적 자원 관리 기법 연구 (메인 후보)**
  - CPU/메모리/GPU/스토리지를 분리(Disaggregated Datacenter, DDC)
  - 필요에 따라 자원을 동적으로 할당·회수
  - 네트워크 지연 문제와 자원 스케줄링 정책 연구


4. **GPU 워크로드 기반 자원 관리 및 스케줄링 최적화 (예비 후보)**
  - GPU 중심의 AI 워크로드 특성을 반영한 운영체제 레벨 자원 관리 연구
  - GPU 활용률을 극대화하기 위한 동적 스케줄링 정책 설계
  - 성능–전력 효율을 동시에 고려한 최적화 기법 개발
  - 대규모 AI 모델 실행 시 GPU 자원 병목 현상 완화 목표


5. **GPU 메모리 상태 기반 실시간 LLM 레이어 재분할 적응형 시스템 (예비 후보)**
  - GPU 메모리 점유율에 따라 LLM(Large Language Model)의 레이어를 실시간으로 재배치/재분할
  - 메모리 부족 상황에서도 안정적으로 대규모 모델 추론 가능
  - 적응형(adaptive) 자원 관리 기법으로 성능–자원 균형 유지
  - GPU 메모리 활용 극대화 및 LLM 추론 효율 향상


6. **[EdgeAI] Neural Network Partitioning for Multi-Access Edge Computing (네트워크 지연과 디바이스 성능에 따라 신경망을 동적으로 분할해서 클라우드-엣지 간 최적 배치)**


7. **[최적화] 여러 추론 작업이 GPU를 공유할 때 실시간 부하에 따라 리소스를 동적 재할당하는 시스템**


8. **[AI(모델학습),시스템 최적화]자체 학습 데이터 생성 및 경량화된 AI 모델을 활용한 '이상 감지 시스템' 개발**


9. **[AI,OS] AI 기반 운영체제 이상 탐지 및 자율 튜닝**


10. **[AI/최적화/분산 스케줄링] AI inference workload에 특화된 serverless 스케줄링 기법 연구(or severless computing 최적화 연구)**
---

## 3. Next Steps
- 팀 내 투표/논의 및 교수님 조언을 통해 메인 주제 확정
- 각 아이디어별 장단점 및 필요 리소스 분석 예정
---

## 4. Feedback on the Topics (Version 0.9)
### 1. AI 모델 경량화
- 이미 많은 연구가 이루어진 분야 → 독창성 확보가 어려움.
- 학부생이 시도하기에는 수월하지만, 좋은 논문 주제로 발전시키기는 부담스러움.

---

### 2. GPU/NPU 아우르는 OS 설계
- 주제가 지나치게 광범위하며, 학부 수준에서 구현은 사실상 불가능에 가까움.
- 졸업 프로젝트 평가에서 좋은 평가를 받기 어려움.

---

### 3. 자원분리형 데이터센터 OS 설계
- 주목받는 분야이긴 하나 학부생에게는 적합하지 않음.
- 구현도 어렵고, 실험으로 결과를 보여주기 힘듦.

---

### 4. GPU 메모리 상태 기반 LLM 레이어 재분할
- 주제 설명 자체는 구체적이고 흥미로움.
- 다만, 구체적으로 어떻게 구현하고 실험할지가 불명확함.

---

### 5. GPU 워크로드 자원 관리 및 스케줄링
- 기본적으로 적절한 주제임.
- 그러나 이미 연구가 많은 분야라 독창성을 확보할 무언가가 필요함.

**정리된 방향**  
- 다양한 AI 모델 학습 시 CPU/GPU/메모리/네트워크 자원 사용 패턴을 수집.  
- 이를 기반으로 자원 사용량 예측 모델을 설계.  
- 성능 지표뿐 아니라 **에너지 지표**도 함께 고려.  

**피드백**  
- 자원 사용량 예측은 이미 많은 연구가 존재.  
- 대신 **에너지 사용량 예측**을 중심으로 연구하면 더 독창적이고 전략적으로 적합.  
- 학부 졸업 프로젝트 주제로도 충분히 feasible하며, 논문 및 최종 평가에서도 경쟁력 있음.

---

### 추가 제안 사항
- 클라우드 서버는 여러 VM이 동시에 AI 워크로드를 실행.  
- AI 워크로드의 에너지 소비는 폭발적으로 증가하고 있음.  
- 따라서 가장 중요한 문제는 **각 VM별 에너지 사용량을 어떻게 추정할 것인가**임.  

**현황**  
- CPU/GPU 단위의 에너지 예측 기술은 존재하지만 정확성이 낮음.  
- 시스템 전체 전력 소모 중 개별 VM의 에너지를 정확히 나누는 연구는 부족함.  

**제안 아이디어**  
- 전체 시스템 전력 소모 데이터를 기반으로, **VM 단위 에너지 사용량을 예측하는 모델** 설계.  
- 전원 단위 측정 장비를 활용 → 모델 예측값과 실제 측정값 비교 검증 가능.  

**확장 가능성**  
- 단순 스케줄링에서 더 나아가, **자원 플래닝/분배 연구**로 확장 가능.  
  - 예: 성능을 일부 낮춰 에너지 절감.  
  - 또는 일정 성능을 보장하면서 에너지 증가를 최소화.
